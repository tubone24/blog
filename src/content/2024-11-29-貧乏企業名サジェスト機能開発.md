---
slug: 2024-11-29/貧乏企業名サジェスト機能開発
title: "企業名サジェスト機能貧乏開発"
date: 2024-11-29T14:24:04+0000
description: お金をかけず企業名サジェストUIを作ろう！
tags:
  - Elasticsearch
headerImage: https://i.imgur.com/1UvKYKu.gif
templateKey: blog-post
---

お金をかけない開発ほど美しいものはないだろう....。

## Table of Contents

```toc

```

## はじめに

おはようございます！！こんにちは！！
この記事はAWS re:invent開催のラスベガスからお送りしてますが、一切AWS関係ないです....!!!

![](https://i.imgur.com/wuH4vwW.jpg)

アドベントカレンダーをre:invent期間中に書こうという魂胆が裏目にでました！

時差ボケで頭が回らない中書いてるので誤字脱字多め。

## 企業検索窓のサジェスト機能が作りたいよー！

突然ですが、こんな感じの企業名を入力することでどんどん企業名の候補がサジェストで出てくるようなUIを作ってほしいとクライアントやプロダクトマネージャーから依頼されたらどうしますか...?

![](https://i.imgur.com/1UvKYKu.gif)

企業名のサジェスト機能を提供しているAPIや実装に組み込めるSDKで提供しているサービスを探したり、（このブログの検索窓でも採用してますが）[Algolia](https://www.algolia.com/)などの全文検索APIサービスを活用するなどを検討することでしょう。

しかしながらそのようなサービスを使って開発を進めることでAWSなどのインフラコストとは別に月額の利用料がかかってしまいます。

個人開発ならまだしも実業務での開発だとこのような交渉は色々な要因で難航することも多いでしょう。

また、企業名など公開されている情報だけでなく、社内で抱えている非公開情報に対してもサジェストをしたい、という場面もあるかもしれません。

そうなってくると、事前に用意されているAPIを使うことはできなくなるわけです。

今回は、 AWSなどのクラウドサービスにスクラッチでサジェスト機能の全文検索の仕組みとフロントエンドを作成し、しかもそのコストをできるだけ下げていく、というなんともマニアックな要件に対して挑んだ軌跡を記載していきたいと思います。

### ちなみに...

企業名に関しては、国税庁が提供する[法人番号システム Web-API](https://www.houjin-bangou.nta.go.jp/pc/webapi/riyokiyaku.html)というものがあります。

商用利用についても「このサービスは、国税庁法人番号システムWeb-API機能を利用して取得した情報をもとに作成しているが、サービスの内容は国税庁によって保証されたものではない」という旨を明記することで利用が可能になります。便利ですね。

ただし...。Web-APIは申請しアプリケーションIDが発行されるまでにいくつか手続きが必要なため、1ヶ月くらい開通までになんだかんだ時間がかかってしまうという問題があります。

今回[法人番号システム Web-API](https://www.houjin-bangou.nta.go.jp/pc/webapi/riyokiyaku.html)を使わず実装したのには上記の待ち時間を持つことが開発上できず、そのような制約下で開発を進める必要があったためです。

## サジェスト機能ってそもそもどうやって作るものなんですかね

まずはサジェスト機能の要件を叶えるためのアーキテクチャを考えていきます。

例えばAlgoliaだと独自に実装された検索エンジンを作成し、パフォーマンスを保っているようですが、要は全文検索エンジンに入力に対してフロントエンドから直接細かく全文検索APIを実行し結果をハイライトする形でサービスを構築することが一般的です。

Algoliaでも直接APIをフロントエンドから叩くことでバックエンドを経由しないことによるパフォーマンスの向上のほか専用のJSライブラリによるInstantSearch, Autocompleteの実装で開発速度の向上が図れることから推奨は直接検索APIを叩く構成とのことです。

[Frontend versus backend search](https://www.algolia.com/doc/guides/building-search-ui/going-further/backend-search/js/)

[What architecture does Algolia use to provide a high-performance search engine?](https://support.algolia.com/hc/en-us/articles/4406975268497-What-architecture-does-Algolia-use-to-provide-a-high-performance-search-engine)

ということで、これをスクラッチで作るとしたらざっくりこんな感じになるはずです。

- 企業名の一覧をどっかから仕入れる
- Elasticsearchのような全文検索エンジンを構築し、サジェストしたい単語（今回は企業名）をインデックスとして登録しておく
- フロントエンドから直接上記のエンジンにクエリを投げ込んで検索結果にアクセスする

![](https://i.imgur.com/OO7JYK3.jpg)

## データソースはどうする？

上記にもちらっと記載しましたが、世の中には[法人番号システム Web-API](https://www.houjin-bangou.nta.go.jp/pc/webapi/riyokiyaku.html)という便利なAPIがありますが、この元ネタになっている情報をCSVやXMLでダウンロードすることができます。[基本３情報ダウンロード](https://www.houjin-bangou.nta.go.jp/download/)

今回の記事では詳しく取り上げませんがこのCSVやXMLには企業名のほか、そのふりがなや住所、倒産や統合・社名変更に関する情報なども含まれております。

### フロントエンド実装

サジェスト機能のフロント側の一番の目玉はインタラクティブに検索結果が検索窓に反映され、Autocompleteする体験だと思います。

Autocompleteの仕組みを一から作るのは大変なのでCSSフレームワークに用意されているAutocompleteを賢く使うことが重要です。

例えばMaterial UIでは[Autocomplete](https://mui.com/material-ui/react-autocomplete/)が用意されているので賢く使って開発速度を落とさないようにしましょう。

上記ではoptionsにサジェストの候補を突っ込むだけでよくあるサジェスト機能が作れます。

![](https://i.imgur.com/1WX3Roe.gif)

### デバウンス処理

上記のMUIの例では、候補のoptionsが固定値ですが実際は入力された値から全文検索のクエリを実行し、結果をoptionsに都度入れ込む必要があります。

ユーザーの入力イベントはonChangeやonInputChangeが設定できるため、都度入力された文字列を取ることができますが、入力イベントが発生するたびにクエリが実行されてしまうと裏側のElasticsearchに大きな負荷がかかります。

そこで採用するのがデバウンス処理です。

デバウンス処理は、短時間に連続して発生する処理を間引くための重要な最適化テクニックです。

よってユーザーの入力がある程度落ち着いたタイミングで一発APIを叩くことで全文検索エンジンに過剰な負荷を防ぐことにつながります。

もちろん全文検索エンジンの性能がよければ、デバウンスでまとめ上げる時間を短くすればするほど、細かくサジェストが反映されて便利ですが、ここは貧乏開発なので、長めに300msくらいに設定しましょう。

デバウンス処理は、lodashのdebounceで次のように比較的かんたんに実装ができます。

```typescript
import { useCallback, useMemo } from "react";

import debounce from "lodash/debounce";

  const searchCompanies = useCallback(
    async (query: string) => {
        // 実際のクエリ処理
    }, [// 実際の依存配列] 
  );

  const debouncedSearch = useMemo(
    () =>
      debounce((query: string) => {
        void searchCompanies(query);
      }, 300), // 300msにした
    [searchCompanies],
  );


```

## 全文検索実装

さて、ここからがこの記事の真骨頂の「如何にして限られたコンピュートリソースで全文検索を実施できるElasticsearchを作るか」という話です。

### AWSならOpensearch serverlessとか使えばいいんじゃない？

御名答。そのとおりです。"お金があれば"。"お金があれば"。"お金があれば"。

インデックス量やワークロードにかかる負荷など前提条件は様々ですが、今回の企業名を検索する用途で使うとインデックス量がFargateのエフェメラルストレージ20GBに収まるため、FargateやFargate spotを使うことで圧倒的に安く作れます。

もちろん可用性を考え冗長な構成を取ったりする必要はあるのであくまでも今回のユースケースではありますし、お金があるのであればマネージドサービスに乗っかったほうが100％よいとは思います。

| デプロイメントタイプ     | インスタンスタイプ  | 月額コスト | 
| ------------------------ | ------------------- | ---------- | 
| 通常のOpenSearch Service | t3.small.search     | $40.992    | 
| Serverless（レプリカ有） | 2 OCU (0.5 × 4)     | $488.976   | 
| Serverless（レプリカ無） | 1 OCU (0.5 × 2)     | $244.488   | 
| ECS Fargate              | (vCPU0.5 / Memory 1G) * 1タスク | $17.77     | 
| ECS Fargate spot         | (vCPU0.5 / Memory 1G) * 1タスク | $5.47      | 

このあと紹介する方法でインデックス作成・クエリ作成していくと、上記のECSのような小さなサイズのタスクでもそこそこいいパフォーマンスで動きます。

### 小さい小さいECSで作ってみよう

先に答えを書きます。

「ワイルドカードを使わず、[N-gram tokenizer](https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-ngram-tokenizer.html)を作って乗り切って」です。

どういうことかというと、ワイルドカード検索という仕組みがElasticsearchにあるのですが、[こちら](https://discuss.elastic.co/t/performance-regression-in-elasticsearch-6-5-using-wildcards/160781)のディスカッションでも話題になってますが、

```json
"query_string": {
  "query": "*sql*"
}
```

のように特に先頭ワイルドカードでクエリを実行すると転置インデックスへの検索に大きな負荷がかかってしまいます。

めちゃくちゃでかいインスタンスやクラスターを組んで実行していけば解決できそうですが、今回のような小さいコンテナで動かすには不向きです。

そこで、企業名を前方から細かく[N-gram tokenizer](https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-ngram-tokenizer.html)を使って転置インデックスを作ってあげるのがよさそうです。

N-gramとは検索ワードを転置インデックスに配置する際に単語を文字数ごとに分割し登録していく方法です。

![](https://i.imgur.com/qH1P0nD.jpg)

こうすることで、「天」や「天下」と入力したタイミングで該当がヒットする（可能性が出てくる）ためすごくサジェストと相性がいいです。

ちなみに、図で緑色で色付けしたように先頭を固定したN-gramを[Edge N-gram](https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-edgengram-tokenizer.html)というまた別のトークナイザーで処理させることで

- 前方一致の検索にはEdge N-gramを利用
- あいまい検索にN-gramを利用
- 両方のインデックスに対してクロスマッチング検索を実施し、Edge N-gramを優先的に評価することで検索精度を上げる

ことができます。

加えてサジェストでは入力途中など漢字への変換が入らない状態で検索が走ることがありますため企業名とは別にふりがなもN-gramで登録しておくことで入力途中にも対応ができます。

（図はEdge N-gramに限定して書いてますがN-gramも同様にインデックスしていきます）

また、入力途中の値が入ってくるパターンと完全に変換しきった文字列が入り乱れるサジェストでは、このN-Gram対象のフィールドを分けてクロスマッチング検索を使うことでよりサジェストチックな検索体験が実現できます。

![](https://i.imgur.com/1HLTLQL.jpg)

インデックスおよびクエリの具体的な指定方法については後ほど細かく見ていきます。

### インフラ構成

今回はこんな感じでECSにすっごいちっちゃいちっちゃいコンテナを立ち上げ、そこであらかじめ企業情報（企業名・住所など）をインデックスしておいたElasticsearchを立ち上げる構成を取ります。

![](https://i.imgur.com/ioPuQHy.jpg)

バックエンドAPIとは切り離しかなり乱暴ですが直接フロントエンドからElasticsearchのクエリを実行させるようにさせてパフォーマンスと追加のバックエンドサーバー構築コストを抑えます。

ただし、そうするとエンドポイントを直接叩き、Elasticsearchのインデックスを好き勝手変更されたり消されるリスクがあるため、このあと後述しますが、インデックス時にインデックスを読み取り専用にしてしまいます。

オンラインでインデックスが更新されることがないのでクラスタ構成とはせず、ちっさいコンテナを必要数（ここでは1個だけ)立ち上げてALBなどでロードバランシングさせる構成となります。

### ベースイメージを用意

今回Elasticsearchのベースイメージは docker.elastic.co/elasticsearch/elasticsearch:7.10.1 を使います。

本来は差分の企業情報が追加されたタイミングでオンラインでインデックスを更新したほうがよいとは思いますが、常に最新の企業情報が反映される必要が要件になかったり、そもそも[基本３情報ダウンロード](https://www.houjin-bangou.nta.go.jp/download/)から順次XMLやCSVをダウンロードする必要があるため、オンラインで該当のインデックスが更新されることがないです。

つまり、コンテナ作成時に追加したインデックスをフリーズして使う、ということです。

ローカル or CI/CDでElasticsearchのイメージを立ち上げ、インデックスを作成後、Docker Commitを使ってイメージを固めてECRなどにPushする構成を取ります。

### スロークエリ監視

それでは早速インデックスを作って行きましょう！

まず、リソース面を削りに削っていくためできる限り小さなコンテナにElasticsearchを展開していくためスロークエリを監視できるようにしておきましょう。

```python
slowlog_template = {
    "settings": {
        "index.search.slowlog.threshold.query.warn": "10s",
        "index.search.slowlog.threshold.query.info": "5s",
        "index.search.slowlog.threshold.query.debug": "2s"
    }
}

es.indices.put_template(name="slowlog-template", body=slowlog_template)
```

このあと作る企業サジェスト用のインデックスに直接設定してもいいですが、テンプレートとして入れておくことですべてのインデックスに設定が反映されるため個人的におすすめです。

### リフレッシュインターバル

```python
refresh_template = {"index_patterns": ["*"], "template": {"settings": {"refresh_interval": "-1", "number_of_replicas": 0}}, "priority": 1}
es.indices.put_index_template(name="default", body=refresh_template)
```

インデックス作成Pythonコードはこんな感じにしております。

```python
import os
import xml.etree.ElementTree as ET

import boto3
import jaconv
from elasticsearch import Elasticsearch
from elasticsearch.helpers import streaming_bulk
from tqdm import tqdm


def clean_value(val):
    if not val or val.isspace():
        return None
    return str(val).strip()


def convert_kana(text):
    if not text:
        return None, None
    hiragana = jaconv.kata2hira(text)
    katakana = jaconv.hira2kata(text)
    return hiragana, katakana


def process_xml_file(xml_file, counters):
    """XMLファイルを少しずつ読み込んで処理する"""
    context = ET.iterparse(xml_file, events=("end",))

    try:
        for event, elem in context:
            # corporationタグの場合のみ処理
            if elem.tag != "corporation":
                continue

            company_name = elem.find("name").text if elem.find("name") is not None else None
            # 非表示企業の除外
            if elem.find("hihyoji") is not None and elem.find("hihyoji").text == "1":
                counters["hidden"] += 1
                elem.clear()
                continue

            # 閉鎖企業の除外
            if elem.find("closeDate") is not None and elem.find("closeDate").text:
                counters["closed"] += 1
                elem.clear()
                continue

            counters["processed"] += 1

            kana = clean_value(elem.find("furigana").text if elem.find("furigana") is not None else None)
            hiragana, katakana = convert_kana(kana)

            doc = {
                "_index": "companies",
                "_source": {
                    "company_name": clean_value(company_name),
                    "tax_id": clean_value(elem.find("corporateNumber").text if elem.find("corporateNumber") is not None else None),
                    "prefecture": clean_value(elem.find("prefectureName").text if elem.find("prefectureName") is not None else None),
                    "city": clean_value(elem.find("cityName").text if elem.find("cityName") is not None else None),
                    "address": clean_value(elem.find("streetNumber").text if elem.find("streetNumber") is not None else None),
                    "postal_code": clean_value(elem.find("postCode").text if elem.find("postCode") is not None else None),
                    "hiragana_name": hiragana,
                    "katakana_name": katakana,
                },
            }

            doc["_source"] = {k: v for k, v in doc["_source"].items() if v is not None}

            if doc["_source"]:
                yield doc

            # メモリ解放のために要素をクリア
            elem.clear()

            # ルート要素の子要素をクリア
            if context.root is not None:
                context.root.clear()

    except Exception as e:
        print(f"XMLファイル処理中にエラー: {e}")


def count_corporations(xml_file):
    """XMLファイル内の法人数をカウント"""
    count = 0
    context = ET.iterparse(xml_file, events=("end",))
    try:
        for event, elem in context:
            if elem.tag == "corporation":
                count += 1
            elem.clear()
    finally:
        if context.root is not None:
            context.root.clear()
    return count


def main():
    xml_files = [f for f in os.listdir(".") if f.endswith(".xml")]

    es = Elasticsearch(["http://localhost:9200"])

    # インデックスのリフレッシュ処理など
    refresh_template = {"index_patterns": ["*"], "template": {"settings": {"refresh_interval": "-1", "number_of_replicas": 0}}, "priority": 1}
    es.indices.put_index_template(name="default", body=refresh_template)

    slowlog_template = {
        "index_patterns": ["*"],
        "settings": {
            "index.search.slowlog.threshold.query.warn": "10s",
            "index.search.slowlog.threshold.query.info": "5s",
            "index.search.slowlog.threshold.query.debug": "2s",
            "index.search.slowlog.threshold.query.trace": "500ms",
            "index.search.slowlog.level": "debug",
        },
    }

    es.indices.put_template(name="slowlog-template", body=slowlog_template)

    index_settings = {
        "settings": {
            "analysis": {
                "analyzer": {
                    "kuromoji_analyzer": {
                        "type": "custom",
                        "tokenizer": "kuromoji_tokenizer",
                        "char_filter": ["icu_normalizer", "kana_converter", "remove_special_chars", "normalize_charset_filter"],
                        "filter": [
                            "lowercase",
                            "kuromoji_baseform",
                            "kuromoji_part_of_speech",
                            "ja_stop",
                            "kuromoji_number",
                            "kuromoji_stemmer",
                            "kana_readingform",
                        ],
                    },
                    "ngram_analyzer": {
                        "type": "custom",
                        "tokenizer": "ngram_tokenizer",
                        "filter": ["lowercase"],
                        "char_filter": ["kana_converter", "remove_special_chars"],
                    },
                    "edge_ngram_analyzer": {
                        "type": "custom",
                        "tokenizer": "edge_ngram_tokenizer",
                        "filter": ["lowercase"],
                        "char_filter": ["kana_converter", "remove_special_chars"],
                    },
                },
                "tokenizer": {
                    "ngram_tokenizer": {"type": "ngram", "min_gram": 2, "max_gram": 3, "token_chars": ["letter", "digit"]},
                    "edge_ngram_tokenizer": {"type": "edge_ngram", "min_gram": 1, "max_gram": 15, "token_chars": ["letter", "digit"]},
                },
                "char_filter": {
                    "kana_converter": {
                        "type": "mapping",
                        "mappings": [
                            # カタカナ→ひらがな（五十音順）
                            "ァ=>ぁ",
                            "ア=>あ",
                            "ィ=>ぃ",
                            "イ=>い",
                            "ゥ=>ぅ",
                            "ウ=>う",
                            "ェ=>ぇ",
                            "エ=>え",
                            "ォ=>ぉ",
                            "オ=>お",
                            "カ=>か",
                            "ガ=>が",
                            "キ=>き",
                            "ギ=>ぎ",
                            "ク=>く",
                            "グ=>ぐ",
                            "ケ=>け",
                            "ゲ=>げ",
                            "コ=>こ",
                            "ゴ=>ご",
                            "サ=>さ",
                            "ザ=>ざ",
                            "シ=>し",
                            "ジ=>じ",
                            "ス=>す",
                            "ズ=>ず",
                            "セ=>せ",
                            "ゼ=>ぜ",
                            "ソ=>そ",
                            "ゾ=>ぞ",
                            "タ=>た",
                            "ダ=>だ",
                            "チ=>ち",
                            "ヂ=>ぢ",
                            "ッ=>っ",
                            "ツ=>つ",
                            "ヅ=>づ",
                            "テ=>て",
                            "デ=>で",
                            "ト=>と",
                            "ド=>ど",
                            "ナ=>な",
                            "ニ=>に",
                            "ヌ=>ぬ",
                            "ネ=>ね",
                            "ノ=>の",
                            "ハ=>は",
                            "バ=>ば",
                            "パ=>ぱ",
                            "ヒ=>ひ",
                            "ビ=>び",
                            "ピ=>ぴ",
                            "フ=>ふ",
                            "ブ=>ぶ",
                            "プ=>ぷ",
                            "ヘ=>へ",
                            "ベ=>べ",
                            "ペ=>ぺ",
                            "ホ=>ほ",
                            "ボ=>ぼ",
                            "ポ=>ぽ",
                            "マ=>ま",
                            "ミ=>み",
                            "ム=>む",
                            "メ=>め",
                            "モ=>も",
                            "ャ=>ゃ",
                            "ヤ=>や",
                            "ュ=>ゅ",
                            "ユ=>ゆ",
                            "ョ=>ょ",
                            "ヨ=>よ",
                            "ラ=>ら",
                            "リ=>り",
                            "ル=>る",
                            "レ=>れ",
                            "ロ=>ろ",
                            "ワ=>わ",
                            "ヲ=>を",
                            "ン=>ん",
                            "ヮ=>ゎ",
                            # 半角カタカナ→ひらがな
                            "ｱ=>あ",
                            "ｲ=>い",
                            "ｳ=>う",
                            "ｴ=>え",
                            "ｵ=>お",
                            "ｶ=>か",
                            "ｷ=>き",
                            "ｸ=>く",
                            "ｹ=>け",
                            "ｺ=>こ",
                            "ｻ=>さ",
                            "ｼ=>し",
                            "ｽ=>す",
                            "ｾ=>せ",
                            "ｿ=>そ",
                            "ﾀ=>た",
                            "ﾁ=>ち",
                            "ﾂ=>つ",
                            "ﾃ=>て",
                            "ﾄ=>と",
                            "ﾅ=>な",
                            "ﾆ=>に",
                            "ﾇ=>ぬ",
                            "ﾈ=>ね",
                            "ﾉ=>の",
                            "ﾊ=>は",
                            "ﾋ=>ひ",
                            "ﾌ=>ふ",
                            "ﾍ=>へ",
                            "ﾎ=>ほ",
                            "ﾏ=>ま",
                            "ﾐ=>み",
                            "ﾑ=>む",
                            "ﾒ=>め",
                            "ﾓ=>も",
                            "ﾔ=>や",
                            "ﾕ=>ゆ",
                            "ﾖ=>よ",
                            "ﾗ=>ら",
                            "ﾘ=>り",
                            "ﾙ=>る",
                            "ﾚ=>れ",
                            "ﾛ=>ろ",
                            "ﾜ=>わ",
                            "ｦ=>を",
                            "ﾝ=>ん",
                            # 濁音、半濁音の処理
                            "ｶﾞ=>が",
                            "ｷﾞ=>ぎ",
                            "ｸﾞ=>ぐ",
                            "ｹﾞ=>げ",
                            "ｺﾞ=>ご",
                            "ｻﾞ=>ざ",
                            "ｼﾞ=>じ",
                            "ｽﾞ=>ず",
                            "ｾﾞ=>ぜ",
                            "ｿﾞ=>ぞ",
                            "ﾀﾞ=>だ",
                            "ﾁﾞ=>ぢ",
                            "ﾂﾞ=>づ",
                            "ﾃﾞ=>で",
                            "ﾄﾞ=>ど",
                            "ﾊﾞ=>ば",
                            "ﾋﾞ=>び",
                            "ﾌﾞ=>ぶ",
                            "ﾍﾞ=>べ",
                            "ﾎﾞ=>ぼ",
                            "ﾊﾟ=>ぱ",
                            "ﾋﾟ=>ぴ",
                            "ﾌﾟ=>ぷ",
                            "ﾍﾟ=>ぺ",
                            "ﾎﾟ=>ぽ",
                            # 記号の正規化（重複を削除）
                            "．=>",
                            "，=>",
                            "、=>",
                            "。=>",
                            "・=>",
                            "･=>",
                            "「=>",
                            "」=>",
                            "『=>",
                            "』=>",
                            "【=>",
                            "】=>",
                            "（=>",
                            "）=>",
                            "(=>",
                            ")=>",
                            "＜=>",
                            "＞=>",
                            # 長音符号の統一
                            "ー=>ー",
                            "―=>ー",
                            "-=>ー",
                            "‐=>ー",
                            "−=>ー",
                            "─=>ー",
                            "━=>ー",
                            "〜=>ー",
                            "～=>ー",
                            "ｰ=>ー",
                            # 空白文字の統一
                            "　=>",  # 全角スペース除去
                        ],
                    },
                    "remove_special_chars": {"type": "pattern_replace", "pattern": "[・．，、。：；！？＆（）｛｝［］【】《》〔〕＜＞「」『』〈〉" "''＝＊＋－∽～①②③④⑤⑥⑦⑧⑨⑩]", "replacement": " "},
                    "normalize_charset_filter": {
                        "type": "mapping",
                        "mappings": [
                            # 全角英字（大文字）を半角に変換
                            "Ａ=>A",
                            "Ｂ=>B",
                            "Ｃ=>C",
                            "Ｄ=>D",
                            "Ｅ=>E",
                            "Ｆ=>F",
                            "Ｇ=>G",
                            "Ｈ=>H",
                            "Ｉ=>I",
                            "Ｊ=>J",
                            "Ｋ=>K",
                            "Ｌ=>L",
                            "Ｍ=>M",
                            "Ｎ=>N",
                            "Ｏ=>O",
                            "Ｐ=>P",
                            "Ｑ=>Q",
                            "Ｒ=>R",
                            "Ｓ=>S",
                            "Ｔ=>T",
                            "Ｕ=>U",
                            "Ｖ=>V",
                            "Ｗ=>W",
                            "Ｘ=>X",
                            "Ｙ=>Y",
                            "Ｚ=>Z",
                            # 全角英字（小文字）を半角に変換
                            "ａ=>a",
                            "ｂ=>b",
                            "ｃ=>c",
                            "ｄ=>d",
                            "ｅ=>e",
                            "ｆ=>f",
                            "ｇ=>g",
                            "ｈ=>h",
                            "ｉ=>i",
                            "ｊ=>j",
                            "ｋ=>k",
                            "ｌ=>l",
                            "ｍ=>m",
                            "ｎ=>n",
                            "ｏ=>o",
                            "ｐ=>p",
                            "ｑ=>q",
                            "ｒ=>r",
                            "ｓ=>s",
                            "ｔ=>t",
                            "ｕ=>u",
                            "ｖ=>v",
                            "ｗ=>w",
                            "ｘ=>x",
                            "ｙ=>y",
                            "ｚ=>z",
                            # 全角数字を半角に変換
                            "０=>0",
                            "１=>1",
                            "２=>2",
                            "３=>3",
                            "４=>4",
                            "５=>5",
                            "６=>6",
                            "７=>7",
                            "８=>8",
                            "９=>9",
                            # 全角記号を半角に変換
                            "＆=>&",
                            "＠=>@",
                            "＃=>#",
                            "％=>%",
                            "＋=>+",
                            "－=>-",
                            "／=>/",
                            "＝=>=",
                        ],
                    },
                },
                "filter": {"kana_readingform": {"type": "kuromoji_readingform", "use_romaji": False}},
                "normalizer": {"lowercase_normalizer": {"type": "custom", "filter": ["lowercase"]}},
            }
        },
        "mappings": {
            "properties": {
                "company_name": {
                    "type": "text",
                    "analyzer": "kuromoji_analyzer",
                    "search_analyzer": "kuromoji_analyzer",
                    "fields": {
                        "keyword": {
                            "type": "keyword",
                            "normalizer": "lowercase_normalizer",
                        },
                        "ngram": {"type": "text", "analyzer": "ngram_analyzer", "search_analyzer": "kuromoji_analyzer"},
                        "edge_ngram": {"type": "text", "analyzer": "edge_ngram_analyzer", "search_analyzer": "kuromoji_analyzer"},
                    },
                },
                "company_name_reading": {
                    "type": "text",
                    "analyzer": "kuromoji_analyzer",
                    "search_analyzer": "kuromoji_analyzer",
                    "fields": {
                        "keyword": {"type": "keyword"},
                        "ngram": {"type": "text", "analyzer": "ngram_analyzer", "search_analyzer": "kuromoji_analyzer"},
                        "edge_ngram": {"type": "text", "analyzer": "edge_ngram_analyzer", "search_analyzer": "kuromoji_analyzer"},
                    },
                },
                "hiragana_name": {
                    "type": "text",
                    "analyzer": "kuromoji_analyzer",
                    "fields": {"keyword": {"type": "keyword", "normalizer": "lowercase_normalizer"}},
                },
                "katakana_name": {
                    "type": "text",
                    "analyzer": "kuromoji_analyzer",
                    "fields": {"keyword": {"type": "keyword", "normalizer": "lowercase_normalizer"}},
                },
                "tax_id": {"type": "text", "fields": {"keyword": {"type": "keyword", "ignore_above": 256}}},
            }
        },
    }

    # インデックスの作成
    if es.indices.exists(index="companies"):
        es.indices.delete(index="companies")
    es.indices.create(index="companies", settings=index_settings["settings"], mappings=index_settings["mappings"])

    es_bulk_chunk_size = 7000
    total_success = 0
    total_errors = []
    counters = {
        "processed": 0,
        "hidden": 0,
        "closed": 0,
    }

    # XMLファイル全体の進捗バー
    with tqdm(total=len(xml_files), desc="XMLファイル処理", unit="ファイル") as file_pbar:
        for xml_file in xml_files:
            try:
                # ESへのバルク登録の進捗バー
                with tqdm(desc=f"ESインデックス中 ({xml_file})", unit="件", leave=False) as bulk_pbar:
                    success = 0
                    for ok, result in streaming_bulk(es, process_xml_file(xml_file, counters), chunk_size=es_bulk_chunk_size, max_retries=3, yield_ok=True):
                        if ok:
                            success += 1
                            bulk_pbar.update(1)
                        else:
                            total_errors.append(result)

                    total_success += success

                file_pbar.update(1)
                file_pbar.set_postfix(
                    {"処理済": counters["processed"], "非表示": counters["hidden"], "閉鎖": counters["closed"], "成功": total_success, "エラー": len(total_errors)}
                )

            except Exception as e:
                print(f"\nファイル {xml_file} でエラーが発生: {e}")

        # インデックスをリフレッシュ
        es.indices.refresh()
        settings_body = {"index": {"blocks": {"write": True, "metadata": True, "read_only": True}, "auto_expand_replicas": "0-all"}}
        response = es.indices.put_settings(index="companies", body=settings_body)
        print("インデックスを読み取り専用に設定しました:", response)


if __name__ == "__main__":
    main()

```

### データクレンジングの工夫

```python
def clean_value(val):
    if not val or val.isspace():
        return None
    return str(val).strip()

def convert_kana(text):
    if not text:
        return None, None
    hiragana = jaconv.kata2hira(text)
    katakana = jaconv.hira2kata(text)
    return hiragana, katakana
```

- 空文字列やスペースのみの値を適切に処理
- ひらがな・カタカナの相互変換による検索精度の向上
- 全角・半角の統一による一貫性の確保

### 日本語検索のための高度な設定


```json
{
  "kuromoji_analyzer": {
    "type": "custom",
    "tokenizer": "kuromoji_tokenizer",
    "char_filter": [
      "icu_normalizer",
      "kana_converter",
      "remove_special_chars",
      "normalize_charset_filter"
    ],
    "filter": [
      "lowercase",
      "kuromoji_baseform",
      "kuromoji_part_of_speech",
      "ja_stop",
      "kuromoji_number",
      "kuromoji_stemmer",
      "kana_readingform"
    ]
  }
}
```

- ICU正規化による文字の標準化
- 特殊文字の除去による検索ノイズの低減
- 日本語ストップワードの除去
- 数字の正規化

### マルチフィールド検索の実装

```json
{
  "company_name": {
    "type": "text",
    "analyzer": "kuromoji_analyzer",
    "fields": {
      "keyword": {
        "type": "keyword",
        "normalizer": "lowercase_normalizer"
      },
      "ngram": {
        "type": "text",
        "analyzer": "ngram_analyzer"
      },
      "edge_ngram": {
        "type": "text",
        "analyzer": "edge_ngram_analyzer"
      }
    }
  }
}
```

- 完全一致検索のためのkeywordフィールド
- あいまい検索のためのN-gram解析
- 前方一致検索のためのEdge N-gram解析
- 各フィールドに適切なアナライザーを設定


