---
slug: 2017/06/04/chainer-chieri-bot
title: 今更！！ Chainer 1.6.1 でおしゃべりBotを作ろう
date: 2017-06-04T02:45:59.982Z
description: 緒方智恵理ちゃんボットなるものを2017年に作った過去記事の移植
tags:
  - Python
  - Chainer
  - RNN
  - LSTM
  - Chat BOT
  - アイマス
  - デレマス
headerImage: 'https://i.imgur.com/DJL1fwu.png'
templateKey: blog-post
---
(過去ブログからの移転した記事です)

なるべくコードはかかない。

何の気なしに機械学習とかの仕事はないものかと、ネットの海をさまよっていたら、

[LSTMで自然な受け答えができるボットをつくった](https://sekailab.com/wp/2015/11/02/lstm-general-responce-bot/) という記事を見つけ、何となく読んで、やってみようかなと思ってやってみましたが、Chianer周りとか色々上手くいかなかった。というのがそもそもの始まりです。

**Chainer周りで日本語の受け答えができますよ**、的な記事は**2015年頃**のものが多く、Chainerがバージョンアップしたため、色々動かないことがありましたので、少しそちらに修正を加えて**Chainer 1.6.1**でもまともに動くように修正していこうと思います。

自分でコードを一から書くのは嫌なので、**あくまでもコードは書かない。**

微修正にとどめるをモットーにがんばるぞい。

## Table of Contents

```toc

```

## 参考にさせていただいたコードやサイト

- [LSTMで自然な受け答えができるボットをつくった](https://sekailab.com/wp/2015/11/02/lstm-general-responce-bot/)
  - このコードを元にあらかじめ作成したChainerモデルでおしゃべりBotに命を吹き込みます。

- [yusuketomoto/chainer-char-rnn](https://github.com/yusuketomoto/chainer-char-rnn)
  - Chainer・日本語界隈では知らない人はいないだろうChainerの言語モデル作成コード。こちらを用いてChainerモデルを作成していきます。

- [Chainerで学習した対話用のボットをSlackで使用+Twitterから学習データを取得してファインチューニング](https://qiita.com/GushiSnow/items/79ca7deeb976f50126d7)
  - 学習に必要なコーパスを取得するために一部のスクリプトを使用します。

## サーバーを用意する

学習にも、会話にも自宅サーバーを用います。

ああ、電気代。

## 学習に必要なPython環境とコーパスを整える

Chainer界隈のコードはなぜかPython2が多く、Python3は少ないので、直接Python2をインストールしてもいいのですが…。

なんかOS環境を変に汚したくないので今回は**PyenvとVirtualenv**を使って実施していきます。

学習用のコーパスをダウンロードするスクリプトはどうやら**Python 3.4.1**で実装されているようなので **Python 3.4.1** と **Python2.7** の2種類の環境を作っていきたいと思います。

そして、3.4.1の環境にて学習用のコーパスをダウンロードします。

PyenvとVirtualenvはあらかじめインストール済みとしてすすめます。

PyenvとVirtualenvのインストール方法はこちらの記事が参考になると思います。[（pyenvとvirtualenvで環境構築）
](https://qiita.com/Kodaira_/items/feadfef9add468e3a85b)

まず、Pythonの環境を作ります。

```shell{promptUser: tubone}{promptHost: dev.localhost}
pyenv install 3.4.1

pyenv install 2.7

pyenv rehash

virtualenv -p ~/.pyenv/versions/3.4.1/bin/python3.4 my_env3.4.1

virtualenv -p ~/.pyenv/versions/2.7/bin/python2.7 my_env2.7
```

続いてコーパスをダウンロードします。

今回は[Chainerで学習した対話用のボットをSlackで使用+Twitterから学習データを取得してファインチューニング](https://qiita.com/GushiSnow/items/79ca7deeb976f50126d7)を参考にダウンロードします。

コーパスデータはたぶん二次配布とかNGだと思うので、何とか自分でダウンロードしてください。

[対話破綻検出チャレンジ](https://sites.google.com/site/dialoguebreakdowndetection/)

ダウンロードしたあとは展開したJSONデータ全部、devフォルダを作成してその中に入れて、listファイル(JSONファイルのパスを記載したやつ)をdata_load.pyと同じところに作っておきます。

こんな感じでlistファイルを作ります。

```shell{promptUser: tubone}{promptHost: dev.localhost}
../dev/1404365812.log.json
../dev/1407143708.log.json
../dev/1407143981.log.json
../dev/1407149923.log.json
../dev/1407208809.log.json
../dev/1407209083.log.json
…… 
```

Linuxのコマンド(bash)で、

`ls -1 ../dev > list`

で作成できるはず。

学習データを作っていきます。

```shell{promptUser: tubone}{promptHost: dev.localhost}
source my_env3.4.1/bin/activate

git clone https://github.com/SnowMasaya/Chainer-Slack-Twitter-Dialogue.git

cd chainer-slack-twitter/utils

python data_load.py
```

player_1.txt , player_2.txtというテキストファイルができます。

統合する前にmecabを使って分かち書きをしておきます。

分かち書きに**mecab-ipadic-NEologd**を使うと学習が進むそうですので入れてないほうは導入しましょう。

めんどくさい人はただの**MeCab**でも大丈夫だと思います。

さて、分かち書きします。今回は **mecab-ipadic-NEologd** を利用します。

```shell{promptUser: tubone}{promptHost: dev.localhost}
mecab –Owakati –d /usr/local/lib/mecab/dic/mecab–ipadic–neologd player_1.txt > player_1_wakati.txt

mecab –Owakati –d /usr/local/lib/mecab/dic/mecab–ipadic–neologd player_2.txt > player_2_wakati.txt
```

次に使うChainer-char-rnn用に1つのinput.txtに統合していきます。統合の際に、player1とplayer2の会話ごとに空行を入れておきます。

```shell{promptUser: tubone}{promptHost: dev.localhost}
paste -d “\n” player_1_wakati.txt player_2_wakati.txt | awk ‘(NR%2==0){$0=$0″\n”}{print}’ > input.txt
```

こちらのinput.txtをコーパスデータとして利用します。

## 学習させる

学習には[yusuketomoto/chainer-char-rnn](https://github.com/yusuketomoto/chainer-char-rnn)を使わせていただきます！

**Chainer 1.4.1** で実行しようとしたら微妙に実装が変わっていた用なので、こちらに合わせて今回の学習は、**Chainer 1.6.1** で実施していきます。(Chainer周りのコードを読んで修正するより、後に使うTornado周りの修正の方がまだわかるからというスキル不足によるもの)

さきほど作っておいたPython 2.7用に切り替えます。PipでChainer1.6.1を入れてからChainer-char-rnnを実行していきます。

```shell{promptUser: tubone}{promptHost: dev.localhost}
source my_env2.7/bin/activate

pip install chainer==”1.6.1″

git clone https://github.com/yusuketomoto/chainer-char-rnn.git

cd chainer-char-rnn

mkdir -p data/chat

mkdir -p cv/chat

cp ../chainer-slack-twitter/input.txt data/chat

python train.py –data_dir data/chat –checkpoint_dir cv/chat –rnn_size 1024
```

しばらく待ちます。全部が終わるのは途方もない時間がかかります。

学習が進むごとにCheckpointとしてChainer modelファイルがcv/chat配下にできますので、適当なEpochのところのものを次の「APIで話す」に使ってもいいですし、学習の最新ファイルである、

latest.chainermodelを使ってもいいです。もちろん、最後まで待ってからlatest.chainermodelを使ってもいいです。

ひとまず数時間回したところのlatest.chainermodelを使ってみます。

## APIで話す

[LSTMで自然な受け答えができるボットをつくった](https://sekailab.com/wp/2015/11/02/lstm-general-responce-bot/)よりJapanese Talk APIを作っていきます。

この回では少々コードの改変がありますのでForkしたものをGitHubにあげました。

[japanese_talk_api_1.6.1](https://github.com/tubone24/japanese_talk_api_chainer14/tree/chainer1.6.1)

こちらのmodelsディレクトリにChainer modelを投入します。

```shell{promptUser: tubone}{promptHost: dev.localhost}
git clone https://github.com/tubone24/japanese_talk_api/tree/chainer1.6.1.git

mkdir japanese_talk_api/tornado/models

cp chainer-char-rnn/cv/chat/latest.chainermodel japanese_talk_api/tornado/models
```

Chainerの他にTornadoも必要になるのでPipでインストールします。

そしてAPIを8787ポートで起動します。

```shell{promptUser: tubone}{promptHost: dev.localhost}
pip install tornado

python japanese_talk_api/tornado/app.py –port=8787 –debug=True
```

あとは起動を待ってからcurlやブラウザなどで、

`http://localhost/?q=こんにちは`

で受け取れるようにはずです。

学習用input.txtにないことばとか出すとたまにエラー吐きます。

## 付属のHubot Scriptで遊んでみよう

[LSTMで自然な受け答えができるボットをつくった](https://sekailab.com/wp/2015/11/02/lstm-general-responce-bot/)のHubotScriptをお借りして遊んでみましょう。

あらかじめ比較として同じコーパスを利用しているDocomoの雑談APIをHubotに仕込んであります。

発言の上が**DocomoAPI** 下が今回作ったAPIです。

ちなみに我が家のHubotはSlack上に「智絵里ちゃん」として君臨しております。

~~智絵里ちゃんマジ天使 **I love you**~~

![img](https://i.imgur.com/DJL1fwu.png)

<!-- textlint-disable -->
**DocomoAPI**に比べると天然というか、不思議系というか…　バカですね（直球）
<!-- textlint-enable -->

お借りした多くのコードや参考にさせていただいた多くのサイト・記事に改めて感謝しつつ、智絵里ちゃんとのラブラブライフを送りますね。
